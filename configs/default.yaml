# File: configs/default.yaml (V11.0: Physico-Semantic & Risk-Seeking Optimized)

model:
  # BERT encoder
  bert_path: "/home/610-sty/huggingface/bert-base-chinese"
  hidden_size: 768
  # Layout decoder
  bb_size: 128            
  decoder_layers: 6
  decoder_heads: 8
  dropout: 0.3
  # Output
  num_classes: 9
  
  # === CVAE Parameters ===
  latent_dim: 64          

  # === LOSS WEIGHTS (SUPERVISED STAGE 1) ===
  # 第一阶段预训练还是用保守的参数，打好基础
  reg_loss_weight: 1.0      
  iou_loss_weight: 1.0      
  area_loss_weight: 1.0     
  
  relation_loss_weight: 5.0 
  overlap_loss_weight: 3.0   
  size_loss_weight: 2.0     
  
  clustering_loss_weight: 1.0
  consistency_loss_weight: 2.0 
  gestalt_reg_loss_weight: 0.5

  # Inference
  max_elements: 30
  
  # Data paths
  xlsx_path: "/home/610-sty/layout2paint/dataset/6800poems.xlsx"
  images_dir: "/home/610-sty/layout2paint/dataset/6800"
  labels_dir: "/home/610-sty/layout2paint/dataset/6800/JPEGImages-pre_new_txt"
  max_layout_length: 30
  max_text_length: 64

training:
  # === Stage 1: Supervised Pre-training ===
  batch_size: 128         
  epochs: 100             
  learning_rate: 0.0001
  warmup_steps: 1000      
  
  # Logging & Saving
  save_every: 50          
  output_dir: "./outputs/train_v11_bold_explore" # [建议] 改个新名字，方便区分
  log_steps: 10
  visualize_every: 5      

  # === Stage 2: RL Fine-tuning Configuration (V11.0 核心) ===
  rl_epochs: 400          
  
  # [CRITICAL] 学习率必须调低！因为 V11 用了指数级激励，梯度很大。
  rl_learning_rate: 2e-6  
  
  # RL Rewards Weights (匹配 V11.0 代码逻辑)
  reward_weights:
    # 1. 基础一致性
    iou: 2.0              
    relation: 3.0  # 稍微加强关系约束       
    
    # 2. 物理与语义锚点 (最重要)
    physics: 4.0          # [NEW] 物理常识：山大鸟小 (V11核心)
    alignment: 5.0        # [UP]  语义位置：跟着热力图走 (强引力)
    heatmap_size: 1.0     # [NEW] 语义大小：热力图辅助微调
    
    # 3. 空间力场 (解决堆叠)
    dispersion: 3.0       # [UP]  分散斥力：把物体炸开 (原0.5太小了)
    overlap: -5.0         # [UP]  重叠惩罚：绝对禁止堆叠 (原-2.0太弱了)
    boundary: -2.0        # [NEW] 边界惩罚：别飞出画布

inference:
  max_output_length: 30