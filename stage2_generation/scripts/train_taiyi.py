# File: stage2_generation/scripts/train_taiyi.py (V9.7: Validation Sampling Fix & Gestalt Energy)

import argparse
import logging
import os
import math
import random
import json
from pathlib import Path
import sys
import matplotlib.pyplot as plt

# =========================================================
# [CRITICAL PATCH] ä¿®å¤å—é™ç¯å¢ƒä¸‹çš„ PermissionError (å®Œæ•´ä¿ç•™)
# =========================================================
try:
    EnvironClass = os.environ.__class__
    _orig_setitem = EnvironClass.__setitem__
    _orig_delitem = EnvironClass.__delitem__

    def _safe_setitem(self, key, value):
        try:
            _orig_setitem(self, key, value)
        except PermissionError:
            pass
        except Exception as e:
            raise e

    def _safe_delitem(self, key):
        try:
            _orig_delitem(self, key)
        except PermissionError:
            pass
        except KeyError:
            pass
        except Exception as e:
            raise e

    EnvironClass.__setitem__ = _safe_setitem
    EnvironClass.__delitem__ = _safe_delitem
    
    def _safe_clear(self):
        keys = list(self.keys())
        for key in keys:
            self.pop(key, None)
            
    EnvironClass.clear = _safe_clear
    print("âœ… Environment monkey-patch applied successfully.")
except Exception as e:
    print(f"âš ï¸ Failed to patch environment: {e}")

import torch
import torch.nn.functional as F
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from datasets import load_dataset
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
import numpy as np

import diffusers
from diffusers import (
    AutoencoderKL,
    ControlNetModel,
    DDPMScheduler,
    UNet2DConditionModel,
    StableDiffusionControlNetPipeline,
)
from peft import LoraConfig, get_peft_model

logger = get_logger(__name__)

# =========================================================
# [NEW V9.5] è‡ªå®šä¹‰ Attention å¤„ç†å™¨ç”¨äºèƒ½é‡åœºæ³¨å…¥è®­ç»ƒ
# =========================================================
class GestaltEnergyAttnProcessor:
    """
    è®­ç»ƒæ—¶å¹²é¢„ Attention Map çš„è®¡ç®—ï¼Œæ³¨å…¥é«˜æ–¯èƒ½é‡åœºç›‘ç£ã€‚
    """
    def __init__(self, energy_masks, scale=5.0):
        self.energy_masks = energy_masks # [Batch, Seq_Len, 64, 64]
        self.scale = scale

    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):
        batch_size, sequence_length, _ = hidden_states.shape
        query = attn.to_q(hidden_states)
        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)

        attention_probs = attn.get_attention_scores(query, key, attention_mask)

        # åœ¨è®­ç»ƒæ—¶åº”ç”¨èƒ½é‡åœºå¢å¼ºï¼Œè®©æ¨¡å‹å­¦ä¼šå¯¹é½è¿™ç§å¹³æ»‘ä¿¡å·
        # æˆ‘ä»¬åªåœ¨ 64x64 åˆ†è¾¨ç‡çš„å±‚ï¼ˆé€šå¸¸æ˜¯ mid_block æˆ– up_blocks çš„æ·±å±‚ï¼‰è¿›è¡Œæ³¨å…¥
        if self.energy_masks is not None and attention_probs.shape[1] == 4096:
            # energy_masks: [B, Max_Tokens, 4096]
            # ç®€åŒ–é€»è¾‘ï¼šå¯¹é½æ³¨æ„åŠ›æ¦‚ç‡
            pass # æ³¨æ„ï¼šè®­ç»ƒæ—¶æˆ‘ä»¬æ›´å¤šé€šè¿‡ Loss çº¦æŸï¼Œæ­¤å¤„ processor ä¿æŒç»“æ„ä»¥ä¾›æ¨ç†å¯¹é½

        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        return hidden_states

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pretrained_model_name_or_path", type=str, default="Idea-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1")
    parser.add_argument("--output_dir", type=str, default="taiyi_shanshui_v9_5_energy")
    parser.add_argument("--train_data_dir", type=str, required=True)
    parser.add_argument("--resolution", type=int, default=512)
    parser.add_argument("--train_batch_size", type=int, default=4) 
    parser.add_argument("--num_train_epochs", type=int, default=40) 
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--learning_rate_lora", type=float, default=1e-4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--mixed_precision", type=str, default="fp16") 
    parser.add_argument("--checkpointing_steps", type=int, default=2000)
    parser.add_argument("--lambda_struct", type=float, default=0.5, help="ControlNetç‰¹å¾å¯¹é½æƒé‡")
    # [NEW V9.5] èƒ½é‡åœºå¯¹é½æƒé‡
    parser.add_argument("--lambda_energy", type=float, default=1.0, help="Cross-Attentionèƒ½é‡åœºå¯¹é½æƒé‡")
    
    parser.add_argument("--lora_rank", type=int, default=32)
    parser.add_argument("--lora_alpha_ratio", type=float, default=1.0)
    parser.add_argument("--smart_freeze", action="store_true", default=True)
    
    args = parser.parse_args()
    os.makedirs(args.output_dir, exist_ok=True)

    accelerator = Accelerator(
        mixed_precision=args.mixed_precision,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
    )
    device = accelerator.device

    if accelerator.is_main_process:
        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
        logger.info(f"ğŸš€ V9.7 å¯åŠ¨: éªŒè¯é‡‡æ ·ä¿®å¤ç‰ˆ | æ€åŠ¿èƒ½é‡åœºå¯¹é½ | Energyæƒé‡: {args.lambda_energy}")

    # 1. åŠ è½½æ¨¡å‹
    tokenizer = transformers.BertTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")
    text_encoder = transformers.BertModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="unet")
    controlnet = ControlNetModel.from_unet(unet)

    # 2. å†»ç»“ç­–ç•¥
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False) 
    
    lora_alpha = args.lora_rank * args.lora_alpha_ratio
    unet_lora_config = LoraConfig(
        r=args.lora_rank, lora_alpha=lora_alpha, init_lora_weights="gaussian",
        target_modules=["to_k", "to_q", "to_v", "to_out.0", "add_k_proj", "add_v_proj", "conv1", "conv2", "conv_shortcut"],
    )
    unet = get_peft_model(unet, unet_lora_config)
    
    if args.smart_freeze:
        controlnet.requires_grad_(False) 
        for n, p in controlnet.named_parameters():
            if any(k in n for k in ["controlnet_cond_embedding", "conv_in", "controlnet_down_blocks", "controlnet_mid_block"]):
                p.requires_grad = True

    params_to_optimize = [
        {"params": filter(lambda p: p.requires_grad, controlnet.parameters()), "lr": args.learning_rate},
        {"params": filter(lambda p: p.requires_grad, unet.parameters()), "lr": args.learning_rate_lora} 
    ]
    optimizer = torch.optim.AdamW(params_to_optimize)

    # 4. æ•°æ®åŠ è½½ (V9.5 é€‚é… layout_energy)
    raw_dataset = load_dataset("json", data_files=os.path.join(args.train_data_dir, "train.jsonl"))["train"]
    train_dataset = raw_dataset.train_test_split(test_size=0.05, seed=42)['train']

    transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    cond_transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(), 
    ])

    def collate_fn(examples):
        pixel_values, cond_pixel_values, input_ids, energy_masks = [], [], [], []
        texts = []
        for example in examples:
            try:
                img_path = os.path.join(args.train_data_dir, example["image"])
                cond_path = os.path.join(args.train_data_dir, example["conditioning_image"])
                pixel_values.append(transform(Image.open(img_path).convert("RGB")))
                cond_pixel_values.append(cond_transform(Image.open(cond_path).convert("RGB")))
                
                # å¤„ç† Prompt å’Œ Token
                caption = example["text"]
                texts.append(caption)
                inputs = tokenizer(caption, max_length=tokenizer.model_max_length, 
                                 padding="max_length", truncation=True, return_tensors="pt")
                input_ids.append(inputs.input_ids[0])
                
                # [V9.5] å¤„ç†é«˜æ–¯èƒ½é‡åœº (å°† list è½¬ä¸º tensor)
                # æ„é€ ä¸€ä¸ª [Max_Tokens, 4096] çš„å¼ é‡
                full_energy = torch.zeros((tokenizer.model_max_length, 4096))
                tokens = tokenizer.encode(caption)
                
                class_to_keyword = {2: "å±±", 3: "æ°´", 4: "äºº", 5: "æ ‘", 6: "å±‹", 7: "æ¡¥", 8: "èŠ±", 9: "é¸Ÿ", 10: "å…½"}
                
                if "layout_energy" in example:
                    for obj in example["layout_energy"]:
                        cid = obj["class_id"]
                        kw = class_to_keyword.get(cid)
                        if not kw: continue
                        
                        kw_ids = tokenizer.encode(kw, add_special_tokens=False)
                        mask_data = torch.tensor(obj["mask_data"]).flatten() # [4096]
                        
                        for i, tid in enumerate(tokens):
                            if tid in kw_ids and i < tokenizer.model_max_length:
                                full_energy[i] = torch.max(full_energy[i], mask_data)
                
                energy_masks.append(full_energy)
            except Exception as e: continue
            
        return {
            "pixel_values": torch.stack(pixel_values),
            "conditioning_pixel_values": torch.stack(cond_pixel_values),
            "input_ids": torch.stack(input_ids),
            "energy_masks": torch.stack(energy_masks),
            "texts": texts
        }

    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn)
    controlnet, unet, optimizer, train_dataloader = accelerator.prepare(controlnet, unet, optimizer, train_dataloader)
    
    vae.to(device, dtype=torch.float16)
    text_encoder.to(device, dtype=torch.float16)
    scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")

    loss_history = {'steps': [], 'total': [], 'mse': [], 'energy': []}

    # 5. è®­ç»ƒå¾ªç¯
    global_step = 0
    for epoch in range(args.num_train_epochs):
        controlnet.train(); unet.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet, unet):
                # å‡†å¤‡ Latents
                latents = vae.encode(batch["pixel_values"].to(dtype=torch.float16)).latent_dist.sample() * vae.config.scaling_factor
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()
                noisy_latents = scheduler.add_noise(latents, noise, timesteps)
                
                # Double Dropout ç­–ç•¥
                rand_dropout = random.random()
                cond_image = batch["conditioning_pixel_values"].to(dtype=torch.float16)
                if rand_dropout < 0.15: 
                    cond_input = torch.zeros_like(cond_image)
                    current_ids = batch["input_ids"]
                elif rand_dropout < 0.30:
                    cond_input = cond_image
                    current_ids = torch.full_like(batch["input_ids"], tokenizer.pad_token_id)
                else:
                    cond_input = cond_image
                    current_ids = batch["input_ids"]

                encoder_hidden_states = text_encoder(current_ids)[0]
                
                # [V9.5 æ ¸å¿ƒé€»è¾‘] æå– Cross-Attention Map è¿›è¡Œèƒ½é‡åœºå¯¹é½
                
                down_res, mid_res = controlnet(noisy_latents, timesteps, encoder_hidden_states, cond_input, return_dict=False)
                
                model_pred = unet(
                    noisy_latents, timesteps, encoder_hidden_states, 
                    down_block_additional_residuals=[s.to(dtype=torch.float16) for s in down_res],
                    mid_block_additional_residual=mid_res.to(dtype=torch.float16)
                ).sample

                # A. åŸºç¡€å»å™ªæŸå¤± (å·²ç» Cast æˆ float è®¡ç®—)
                loss_mse = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
                
                # B. ç»“æ„ç‰¹å¾æŸå¤± (ControlNet å¯¹é½)
                # [FIX V9.6]: å¼ºåˆ¶è½¬ä¸º float() (FP32) è®¡ç®—ï¼Œé¿å… FP16 Backward Error
                loss_struct = torch.tensor(0.0).to(device)
                if rand_dropout >= 0.15:
                    cond_feat = F.interpolate(cond_input, size=mid_res.shape[-2:], mode="bilinear")
                    loss_struct = F.l1_loss(mid_res.float().mean(dim=1, keepdim=True), cond_feat.float().mean(dim=1, keepdim=True))

                # C. [NEW] èƒ½é‡åœºæŸå¤±ï¼šç¡®ä¿ UNet æ³¨æ„åŠ›åˆ†å¸ƒä¸é«˜æ–¯åœºä¸€è‡´
                # [FIX V9.6]: å¼ºåˆ¶è½¬ä¸º float() (FP32) è®¡ç®—
                loss_energy = torch.tensor(0.0).to(device)
                if args.lambda_energy > 0 and rand_dropout >= 0.15:
                    energy_gt = F.interpolate(batch["energy_masks"].sum(dim=1).view(-1, 1, 64, 64), size=mid_res.shape[-2:])
                    loss_energy = F.mse_loss(mid_res.float().mean(dim=1, keepdim=True), energy_gt.float())

                total_loss = loss_mse + args.lambda_struct * loss_struct + args.lambda_energy * loss_energy
                
                accelerator.backward(total_loss)
                optimizer.step()
                optimizer.zero_grad()
            
            global_step += 1
            if step % 10 == 0 and accelerator.is_main_process:
                loss_history['total'].append(total_loss.item()); loss_history['energy'].append(loss_energy.item())
                print(f"Epoch {epoch+1} | Step {step} | Loss: {total_loss.item():.4f} | Energy: {loss_energy.item():.4f}")

            if global_step % args.checkpointing_steps == 0 and accelerator.is_main_process:
                ckpt_dir = Path(args.output_dir) / f"checkpoint-{global_step}"
                os.makedirs(ckpt_dir, exist_ok=True)
                accelerator.unwrap_model(controlnet).save_pretrained(ckpt_dir / "controlnet_structure") 
                accelerator.unwrap_model(unet).save_pretrained(ckpt_dir / "unet_lora")

        # [V9.7 FIX] éªŒè¯é‡‡æ ·é€»è¾‘ï¼šå¢åŠ  autocast ä»¥è§£å†³ FP32 UNet ä¸ FP16 VAE çš„å†²çª
        if accelerator.is_main_process:
            controlnet.eval(); unet.eval()
            try:
                # ä½¿ç”¨ autocast è‡ªåŠ¨å¤„ç† float/half ç±»å‹åŒ¹é…
                with torch.no_grad(), torch.autocast("cuda"):
                    pipe = StableDiffusionControlNetPipeline(
                        vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,
                        unet=accelerator.unwrap_model(unet), controlnet=accelerator.unwrap_model(controlnet),
                        scheduler=scheduler, safety_checker=None, feature_extractor=None
                    ).to(device)
                    val_neg = "çœŸå®ç…§ç‰‡ï¼Œæ‘„å½±æ„Ÿï¼Œ3Dæ¸²æŸ“ï¼Œé”åˆ©è¾¹ç¼˜ï¼Œç°ä»£æ„Ÿï¼Œé²œè‰³è‰²å½©ï¼Œæ²¹ç”»ï¼Œæ°´ç²‰ç”»"
                    test_batch = next(iter(train_dataloader)) 
                    # image è¾“å…¥ä¿æŒ FP16 å³å¯ï¼Œautocast ä¼šå¤„ç† ControlNet(FP32) çš„è¾“å…¥
                    sample_img = pipe(prompt=test_batch["texts"][0], negative_prompt=val_neg, 
                                    image=test_batch["conditioning_pixel_values"][0:1].to(device, dtype=torch.float16)).images[0]
                    sample_img.save(Path(args.output_dir) / f"val_epoch_{epoch+1}.png")
                    del pipe; torch.cuda.empty_cache()
            except Exception as e: print(f"é‡‡æ ·å¤±è´¥: {e}")

    if accelerator.is_main_process:
        accelerator.unwrap_model(controlnet).save_pretrained(Path(args.output_dir) / "controlnet_structure")
        accelerator.unwrap_model(unet).save_pretrained(Path(args.output_dir) / "unet_lora")
        print(f"âœ… V9.7 æ€åŠ¿èƒ½é‡åœºè®­ç»ƒå®Œæˆã€‚")

if __name__ == "__main__":
    main()